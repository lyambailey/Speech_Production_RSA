{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this script is to parse output from PsychoPy into fMRI log files (event timing files for each condition / stimulus, which can later be fed to FEAT). You must run this script individually for each participant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_rows', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define top dir\n",
    "topDir = open('../top_dir_win.txt').read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify subject and input data\n",
    "subj = \"subject-030\" \n",
    "\n",
    "inFilePath = topDir + \"/behavioural_data/fmri_runs2/\" + subj + '/'\n",
    "\n",
    "# # Specify directories for output (and make directories if they don't alraedy exist)\n",
    "outFilePath = topDir + \"/MRIanalyses/\" + 'assets/' + subj + '/' + subj + \"_log_files/\"\n",
    "if not os.path.exists(outFilePath):\n",
    "    os.makedirs(outFilePath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load behavioural data from each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns we want to read in\n",
    "studyCols = ['cond','studied_word', 'studyWord.started']\n",
    "\n",
    "# The easiest way to deal with multiple datasets is to define a function for reading in the data\n",
    "def readData(run_n):\n",
    "    inFile = inFile1 = inFilePath + subj + \"_quickrun\" + run_n + \".csv\"\n",
    "    df = pd.read_csv(inFile, usecols = studyCols)\n",
    "    \n",
    "    # Remoave NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Rename columns for convenience \n",
    "    df.rename(columns={'cond':'CONDITION', 'studied_word':'WORD', 'studyWord.started': 'WORDON'}, inplace=True)\n",
    "    \n",
    "    # Add run #\n",
    "    df['RUN'] = run_n\n",
    "    \n",
    "    return df\n",
    "\n",
    "df1 = readData('1')\n",
    "df2 = readData('2')\n",
    "df3 = readData('3')\n",
    "df4 = readData('4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust stimulus timing\n",
    "Note that stim timings are relative to hitting \"start\" in PsychoPy, however there is a variable (uncontrolled) period between \"start\" and the actual onset of trails/scans, because the experiment begins with an instruction/\"please wait\" screen. Therefore, we need to make timings relative to the onset of trials, which was triggered by the MRI tech hitting \"s\" as they initiated the scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to fix stim timing\n",
    "def correctTimings(run_n):\n",
    "\n",
    "    # Read in log file (which tells us exactly when the \"s\" was pressed, relative to PsychoPy starting)\n",
    "    log_file = inFilePath + subj + '_quickrun' + run_n + '.log'\n",
    "    \n",
    "    \n",
    "# Read in as list (each line is an element)\n",
    "    f = open(log_file)\n",
    "    log = f.readlines()\n",
    "    \n",
    "    # Pull out the line containing \"Keypress: s\" (there should only be one instance in the whole run)\n",
    "    for i in log:\n",
    "        if \"Keypress: s\" in i:\n",
    "#             print(i)\n",
    "            startLine = i\n",
    "    \n",
    "    # Extract timing\n",
    "    absStart = float(startLine.split(\"\\t\")[0])\n",
    "    \n",
    "    # Call the df for this run\n",
    "    df = eval('df' + run_n)\n",
    "    \n",
    "    # Subtract the absolute start time from WORDON. Also subtract the time for dummy scans (5 TRs, with each TR being 1.8 s)\n",
    "    df['WORDON_adj'] = df['WORDON'] - (absStart + (1.8 * 5))\n",
    "    \n",
    "    # Assuming we will concatenate runs, we need to adjust the timing of runs 2, 3, and 4 as if these trials began immediately after the last run.\n",
    "    # Each run contains 100 TRs of 1.8 s. Therefore we need to add 180 s for every preceding run.\n",
    "    n_preceding = float(run_n)-1\n",
    "    df['WORDON_4concat'] = df['WORDON_adj'] + (180*n_preceding)\n",
    "    \n",
    "    # Return the adjusted dataframe\n",
    "    return df\n",
    "            \n",
    "# Use the function to adjust stim timing in each df. Note that the first timepoint should always begin at \n",
    "# approximately 1.5 seconds, because the first cue (which ought to start at timepoint 0) lasts 1.5 seconds.\n",
    "df1_adj = correctTimings('1')\n",
    "df2_adj = correctTimings('2')\n",
    "df3_adj = correctTimings('3')\n",
    "df4_adj = correctTimings('4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look\n",
    "# df2_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the df's from the 4 runs\n",
    "df = pd.concat([df1_adj,df2_adj,df3_adj,df4_adj])\n",
    "\n",
    "# Add word duration\n",
    "df['wordDuration'] = 2.5\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(axis=1,labels='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality checking! \n",
    "Confirm that each word appears 4 times, and that each word is always in the same condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that df contains 120 rows (30 words X 4 runs)\n",
    "if len(df)==120:\n",
    "    print(\"120 rows in df - all good!\")\n",
    "else:\n",
    "    print(\"WARNING - there are\", len(df), \"rows in df!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that every word occurs 4 times (will only show printout if there is a problem)\n",
    "for i in list(df['WORD']):\n",
    "    if list(df['WORD']).count(i)!=4:\n",
    "        print('WARNING: ', i, \"appears\", list(df['WORD']).count(i), \"times!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that every word is always in the same condition. \n",
    "\n",
    "# To do this, we need to make a copy of df containing only WORD and CONDITION\n",
    "df_debug = df[['WORD','CONDITION']].copy()\n",
    "\n",
    "# Optional: Try artificially messing with word-condition mapping, to test whether this works \n",
    "# df_debug.CONDITION.iloc[[0,5,7,11,19,55]] = 'aloud'\n",
    "\n",
    "\n",
    "x = df_debug.loc[~df_debug.sort_values([\"WORD\", \"CONDITION\"]).duplicated(keep=False)]\n",
    "\n",
    "if x.empty:\n",
    "    print(\"Each word is always in the same condition\")\n",
    "else:\n",
    "    print(\"WARNING:\",len(x),\" word(s) appeared in two conditions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also check manually, if desired\n",
    "# df.sort_values([\"WORD\",\"CONDITION\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate condition-wise output log files for FSL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through runs and conditions\n",
    "conditions = ['aloud','silent']\n",
    "runs = ['1','2','3','4']\n",
    "\n",
    "for run in runs:\n",
    "    for cond in conditions:\n",
    "        dfTmp = df[df['CONDITION'].isin([cond]) & df['RUN'].isin([run])][['WORDON_adj','wordDuration']]\n",
    "        dfTmp['col3'] = 1\n",
    "        dfTmp.to_csv(outFilePath + '/' + subj + '_quickread' + str(run) + '_' + cond + '_alltrials.txt', sep='\\t', header=False, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do the same for concatenated runs\n",
    "# for cond in conditions:\n",
    "#     dfTmp = df[df['CONDITION'].isin([cond])][['WORDON_4concat','wordDuration']]\n",
    "#     dfTmp['col3'] = 1\n",
    "#     dfTmp.to_csv(outFilePath + '/' + subj + '_quickread_concat' + '_' + cond + '.txt', sep='\\t', header=False, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate trial-wise output log files for FSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the list of words, saving a separate log file for each word/run combination. \n",
    "wordList = list(set(df['WORD']))\n",
    "\n",
    "for word in wordList:\n",
    "    for cond in conditions:\n",
    "        for run in runs:\n",
    "            dfTmp = df[df['CONDITION'].isin([cond]) & df['RUN'].isin([run]) & df['WORD'].isin([word])][['WORDON_adj','wordDuration']]\n",
    "            \n",
    "            # Python will try to make a file for every possible combination of word, condition, and run. This means we get 2X the number of\n",
    "            # files we want, because it tries to match every word to both silent and aloud, resulting in one empty dfTmp for every word. To avoid\n",
    "            # this, skip every iteration where dfTmp is empty.\n",
    "            if dfTmp.empty:\n",
    "                continue\n",
    "            \n",
    "            dfTmp['col3'] = 1\n",
    "            dfTmp.to_csv(outFilePath + '/' + subj + '_quickread' + str(run) + '_' + cond + '_' + word + '.txt', sep='\\t', header=False, index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over words again, but this time save a log file for each word, containing times for all 4 runs\n",
    "# for word in wordList:\n",
    "#     for cond in conditions:\n",
    "#         for run in runs:\n",
    "#             dfTmp = df[df['CONDITION'].isin([cond]) & df['WORD'].isin([word])][['WORDON_4concat','wordDuration']]\n",
    "            \n",
    "#             # Python will try to make a file for every possible combination of word & condition. This means we get 2X the number of\n",
    "#             # files we want, because it tries to match every word to both silent and aloud, resulting in one empty dfTmp for every word. To avoid\n",
    "#             # this, skip every iteration where dfTmp is empty.\n",
    "#             if dfTmp.empty:\n",
    "#                 continue\n",
    "            \n",
    "#             dfTmp['col3'] = 1\n",
    "#             dfTmp.to_csv(trialWisePath_concat + '/' + subj + '_quickread_concat' + '_' + cond + '_' + word + '.txt', sep='\\t', header=False, index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nteract": {
   "version": "0.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
